{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Keras Tuner for the DNN\n",
    "\n",
    "We performed hyperparameter tuning to find the optimal number of neurons in each dense layer.\n",
    "\n",
    "Best results: 320 for first layer and 224 for the second layer.\n",
    "\n",
    "We then use this in the final DNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import keras_tuner as kt\n",
    "from utils.data import load_and_preprocess_data\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from utils.constants import NUMERICAL_FEATURES, RANDOM_SEED\n",
    "from utils.plots import (\n",
    "    plot_training_validation_auc,\n",
    "    plot_training_validation_loss,\n",
    "    plot_roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = RANDOM_SEED  # Random seed to ensure reproducibility\n",
    "output_path = \"output/tf_hyperparameter_tuning/\"\n",
    "cmap = \"seismic\"  # Colormap for SHAP plots use \"seismic\" for full cohort and \"berlin\" for biochem remission cohort\n",
    "file_prefix = \"tensorflow\"\n",
    "# file_prefix = \"biochem_remission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardised Data Loading, Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_data() # pass biochem=True to use only biochem remission cohort\n",
    "\n",
    "# Optimisation\n",
    "columns_to_drop = [\n",
    "    \"has_active_symptoms\",\n",
    "    \"baseline_eims_arthralgia_arthritis\",\n",
    "    \"baseline_eims_ankylosing_spondylitis\",\n",
    "    \"baseline_eims_erythema_nodosum\",\n",
    "    \"baseline_eims_uveitis\",\n",
    "    \"baseline_eims_scleritis_episclerities\",\n",
    "    \"is_smoker_smokeryn1\",\n",
    "    \"study_group_name_Await Dx\",\n",
    "    \"ifx_drug_level\",\n",
    "    \"ada_drug_level\",\n",
    "    \"ifx_drug_level_present\",\n",
    "    \"ada_drug_level_present\",\n",
    "    \"ifx_antibody_present\",\n",
    "    \"ada_antibody_present\",\n",
    "    \"haematocrit\",\n",
    "]\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "numerical_features = NUMERICAL_FEATURES\n",
    "numerical_features = [col for col in numerical_features if col not in ['haematocrit', 'ada_drug_level', 'ifx_drug_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train Validate and Test Datasets\n",
    "\n",
    "# First split into train and temp 70% train, 30% temp which will be split 50:50 into 15% val and 15% test\n",
    "\n",
    "# GroupShuffleSplit\n",
    "splitter = GroupShuffleSplit(test_size=0.36, n_splits=1, random_state=random_seed)\n",
    "\n",
    "# Perform the split\n",
    "for train_idx, test_idx in splitter.split(df, groups=df[\"study_id\"]):\n",
    "    train_data = df.iloc[train_idx]\n",
    "    temp_data = df.iloc[test_idx]\n",
    "\n",
    "# Drop 'study_id' from X_train and X_test as it's not a feature\n",
    "X_train = train_data.drop(columns=[\"fatigue_outcome\", \"study_id\"])\n",
    "y_train = train_data[\"fatigue_outcome\"]\n",
    "\n",
    "groups = train_data[\"study_id\"]  # Group variable for GroupKFold cross-validation\n",
    "\n",
    "temp_data_splitter = GroupShuffleSplit(\n",
    "    test_size=0.56, n_splits=1, random_state=random_seed\n",
    ")\n",
    "\n",
    "# Perform the split\n",
    "for val_idx, test_idx in temp_data_splitter.split(\n",
    "    temp_data, groups=temp_data[\"study_id\"]\n",
    "):\n",
    "    val_data = df.iloc[val_idx]\n",
    "    test_data = df.iloc[test_idx]\n",
    "\n",
    "X_val = val_data.drop(columns=[\"fatigue_outcome\", \"study_id\"])\n",
    "y_val = val_data[\"fatigue_outcome\"]\n",
    "\n",
    "X_test = test_data.drop(columns=[\"fatigue_outcome\", \"study_id\"])\n",
    "y_test = test_data[\"fatigue_outcome\"]\n",
    "\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Val shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Y train shape:\", y_train.shape)\n",
    "print(\"Y val shape:\", y_val.shape)\n",
    "print(\"Y test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical features using StandardScaler\n",
    "X_unified = pd.concat([X_train, X_val, X_test])\n",
    "unified_scaler = StandardScaler()\n",
    "unified_scaler.fit(X_unified[numerical_features])\n",
    "\n",
    "X_train[numerical_features] = unified_scaler.transform(X_train[numerical_features])\n",
    "X_test[numerical_features] = unified_scaler.transform(X_test[numerical_features])\n",
    "X_val[numerical_features] = unified_scaler.transform(X_val[numerical_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    number_of_layers = hp.Int(\"number_of_layers\", min_value=1, max_value=2, step=1)\n",
    "    for i in range(number_of_layers):\n",
    "        hp_units = hp.Int(\"units_\" + str(i), min_value=256, max_value=400, step=32)\n",
    "        model.add(keras.layers.Dense(units=hp_units, activation=\"relu\"))\n",
    "        hp_dropout = hp.Float(\"dropout_\" + str(i), min_value=0.2, max_value=0.4, step=0.1)\n",
    "        model.add(keras.layers.Dropout(hp_dropout))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[AUC()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective=\"val_auc\",\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory=\"working_data\",\n",
    "    project_name=\"keras_tuner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal hyperparameters are:\n",
    "- Number of layers: {best_hps.get('number_of_layers')}\n",
    "\"\"\")\n",
    "for i in range(best_hps.get(\"number_of_layers\")):\n",
    "    print(f\"Layer {i+1}: Units: {best_hps.get('units_' + str(i))}, Dropout: {best_hps.get('dropout_' + str(i))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_validation_auc(history_dict, output_path, file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_validation_loss(history_dict, output_path, file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "y_classes = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_classes).ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_classes)\n",
    "sensitivity = recall_score(y_test, y_classes)  # TPR\n",
    "specificity = tn / (tn + fp)  # TN\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"AUC:\", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(output_path + \"dnn_fpr.txt\", fpr)\n",
    "np.savetxt(output_path + \"dnn_tpr.txt\", tpr)\n",
    "\n",
    "with open(output_path + \"dnn_auc.txt\", \"w\") as f:\n",
    "    f.write(str(test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, test_auc, output_path, file_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
